
\section{Summary of contribution}

\emph{Safety} is a syntactic constraint for higher-order grammars.
A grammar is \emph{safe} if the right-hand side of each rule is such that no subterm
occurring in operand position contains parameters of order smaller than
the order of the subterm. Motivated by the appealing algorithmic properties of safety, we derived a new typing system, the \emph{safe lambda calculus}, by imposing
this syntactic constraint on the simply typed lambda calculus.
The salient property of this calculus is that it is not necessary to rename variables when performing substitution.
Thus in some sense, safe terms are ``easier'' to compute than unsafe ones.
Computation in our calculus is standardly done via the concept of $\beta$-reduction.
 Safety is not preserved by beta-reduction in general, but it is preserved when sufficiently many consecutive redexes are contracted simultaneously. This is formalized by the notion of \emph{safe beta-reduction}: If a safe term contains a $\beta$-redex then this redex can always be ``enlarged'' into a group of consecutive beta-redexes, called a safe redex, such that contracting all of them produces a safe term. The notion of normal form thus remains unchanged. Further, safety is an extensional property: a term is safe if and only if its eta-long normal form is.

The typing system of the safe lambda calculus has desirable properties: the
type-checking (Can a given type be assigned to a given term?)
and typability (Given a term, is there a type that can be assigned to it?) problems are both decidable.
On the other-hand, we only know that the type-inhabitation problem
(Given a type, is there a safe term of that type?) is at least semi-decidable (there is an algorithm that tells if a type is inhabited by a safe term in a finite amount of time if it is the case, but may not terminate otherwise).

The loss of expressivity incurred by safety can be characterized in terms of expressible numeric functions:
they are precisely the multivariate polynomials; the conditional operator, which is definable in the lambda calculus, is not expressible by any safe term. In terms of representable word functions, these are given by the set containing the projections, constant functions, concatenation and substitution and closed by composition.

We then looked at the complexity of the calculus by considering the beta-equivalence problem:
we hinted that it probably lies in the complexity class ELEMENTARY by showing how both Statman and Mairson's encoding of finite type theory in the simply typed lambda calculus fail in the safe fragment.
We showed however that the problem is PSPACE-hard.
\medskip

Seeking a semantic explanation of the safety constraint, we focused on the analysis of the game semantics of safe terms.
This led us to the other main contribution of this thesis: the development
of a new presentation of Game semantics based on the theory of traversals \cite{OngLics2006}.
Essentially, traversals implement a version of $\beta$-reduction in which beta-redex are computed locally
as opposed to a global approach based on substitution. The soundness of the traversal theory as a model
of computation is ensured by the correspondence with game semantics:
traversals are just uncovering of plays in game semantics.


Armed with the Correspondence Theorem, we were able to give a precise account of the game
semantics of the safe lambda-calculus. A notable property of safe terms is that its variables are incrementally-bound:
the binder of a variable node $x$ in the computation tree is precisely the last lambda-node in the path from $x$ to the root with order strictly greater than $\ord x$. By the Correspondence Theorem, this implies that the strategy denotation of a safe term is \emph{P-incrementally justified}. In such strategy, a P-question's justifier is given by the last O-move in the P-view with greater order.

In the last chapter we finally investigated the categorical model of the safe lambda
calculus. We proposed the notion of Incremental Closed Category (ICC) that soundly interprets the safe lambda calculus in the same way Cartesian Closed Categories model the simply typed lambda calculus. We then exhibited such an ICC by constructing a game model of P-incrementally justified strategies. (We showed in particular that P-incremental justified strategies compose.)

To conclude, we looked at safety from the point of view of \emph{Algorithmic Game Semantics}. We considered the problem of observational equivalence of \ialgol\ term with respect to \emph{safe} contexts.
By suitably constraining O-moves by the dual notion of \emph{O-incremental justification}, we
obtain a model of safe PCF and safe \ialgol\ that is fully abstract with respect to
this notion of observational equivalence. Furthermore, the model is effectively presentable: two safe terms are observationally equivalent (with respect to safe contexts) if and only if their denotations have the same set of \emph{complete O-incrementally justified} plays.

Up to order $3$, the addition of unsafe contexts to safe ones is conservative with respect to observational equivalence. Furthermore, all the complexity results---lower and upper bounds---known about observational equivalence of the (unrestricted) lower-order fragments of \ialgol\ also hold in the safe sub-fragments. At order-$4$, however, the notion of observational equivalence with respect to unrestricted contexts differs from the one defined with respect to safe contexts only.
Concerning the latter, we conjecture that the restriction of the problem to \emph{safe} terms (\ie, safe observational equivalence of safe $\ialgol_4$ terms) is reducible to the DPDA-equivalence problem (which is decidable).



\section{Further works}

The nature of the safe lambda calculus is still not completely
understood. Some questions remain about the safe lambda-calculus pertaining for instance to its computational power, the complexity classes that it characterizes and its interpretation under the Curry-Howard isomorphism.
We now propose possible directions for further works and highlight some open questions.

\subsection*{Type theory}

One of the most pressing open problems concerns the complexity of the safe lambda calculus.
We have shown that the beta-equivalence problem is PSPACE-hard, but this lower-bound may be very coarse. Further investigations need to be done to determine an upper-bound.

Another open problem is the question of decidability of type inhabitation. At the moment we already know that it is semi-decidable: there is an algorithm that, given a simple type, can exhibit a safe inhabitant if it exists but may not terminate otherwise.

\subsection*{Extensions}

We have defined a notion of safety for simply typed terms (and also for untyped terms by means of a Curry-like version of the typing system). Is there any generalization to more complicated typing system such as the second-order lambda calculus?

\subsection*{Logic}
What kind of reasoning principles does the safe lambda calculus support via the Curry-Howard Isomorphism? How expressive is the safe fragment of intuitionistic implication logic? Is the logic decidable?---or equivalently is type inhabitation decidable in the safe lambda calculus?

\subsection*{Computational complexity}

Can the safe lambda calculus help to characterize complexity classes?
There are already many such results in the unrestricted case: Leivant and Marion \cite{DBLP:conf/tlca/LeivantM93} considered for instance an ``impure'' variation of the simply typed lambda calculus extended with constructors, destructors and conditionals, and obtain several characterization of the polytime-computable numeric functions in that language.

Hillebrand, Kanellakis and Mairson \cite{DBLP:journals/iandc/HillebrandKM96} considered the problem from a database point of view. Instead of encoding numeric functions, they looked at the database queries that are encodable in the simply typed lambda calculus and gave a precise characterization of PTIME: the polynomial time queries are those expressible in the 4th order fragment of the simply typed lambda calculus.
This result was later extended to give characterizations of the standard complexity classes PSPACE, $k$-EXPTIME, $k$-EXPSPACE ($k\geq1$) and ELEMENTARY
at higher-orders \cite{DBLP:conf/lics/HillebrandK96}.

More research needs to be done to see if similar characterizations can be obtained in the safe lambda calculus.


% (it is unlikely to obtain the complexity PSPACE because the
% set of complete plays of the safe term $\lambda f^{(o,o),o} . f
% (\lambda x^o . x)$ is not regular \cite{DBLP:journals/apal/Ong04}).


\subsection*{Expressibility}

\subsubsection*{Functions over free algebras}

\emph{What are the function over \emph{free-algebras} definable in the safe simply typed lambda calculus?}

There is an isomorphism between binary trees and closed simply typed terms of
type $\tau =(o\typear o\typear o) \typear o \typear o$. Thus any
closed term of type $\tau\typear\tau \typear \ldots \typear \tau $
represents an $n$-ary function over trees. Zaionc \cite{DBLP:conf/aluacs/Zaionc88} and Leivant \cite{DBLP:journals/tcs/Leivant93} gave a characterization of the set of tree functions representable in the simply typed lambda calculus: it is
precisely the minimal set containing constant functions, projections
and closed under composition and limited primitive recursion. Zaionc
showed that the same characterization holds for the general case of
functions expressed over free algebras
\cite{DBLP:journals/apal/Zaionc91}: they are given by the
minimal set containing constant functions, projections and closed
under composition and limited primitive recursion. This result subsumes
Schwichtenberg's result on definable numeric functions as well as
Zaionc's own results on definable word and tree functions.

All these basic operations are safe except limited primitive recursion.
This suggests that one needs to restrict further the primitive recursion in order to obtain a characterization of free-algebra functions representable in the
safe lambda calculus. Such result would generalize our expressivity result for numeric and word functions from Sec.\ \ref{sec:expressivity}.

\subsubsection*{Murawski-expressibility}

Murawski introduced a notion of language expressibility by game semantics \cite{Murawski2003}. He showed that the $4th$ order finitary fragment of \ialgol\ is expressive enough to give the full class of recursively enumerable languages. Does the safe fragment have the same expressive power? Another line of research would be to investigate whether the class of word languages recognizable by higher-order pushdown automata can be characterized in Murawski's sense by some higher-order fragment of safe \ialgol.

\subsubsection*{Trees and word languages}

The impact of safety on the expressivity of higher-order recursion schemes was studied in de Miranda's thesis \cite{demirandathesis}. At order $2$ and for word languages, safety is not a genuine constraint if we allow non-determinism \cite{DBLP:conf/fossacs/AehligMO05}; de Miranda and Urzyczyn conjectured that for \emph{deterministic} higher-order grammars, safety is a proper restriction.
Urzyczyn even proposed an unsafe deterministic higher-order recursion scheme generating a word language that he conjectured to be inherently unsafe (\ie, that cannot be generated by any deterministic safe grammar). At the time of this writing, though, this remains a conjecture. The traversal theory seems to be a promising tool to investigate the problem.


\subsection*{Game semantics}

Is the game model of safe \pcf\ universal? (\ie, is every recursive incremental strategy denoted by some safe \pcf\ term?) Is there a category of O-incrementally justified strategies?

\subsection*{Compilation of safe recursion schemes to pushdown automata}

We have mentioned before the equi-expressivity result about safe homogeneously-typed higher-order recursion schemes and higher-order pushdown automata: these two devices generate the same class of infinite trees. Hague et~al.\ generalized this result to unrestricted recursion scheme; one direction relies on the traversal theory: an order $n$ recursion scheme can be compiled into an equivalent order $n$ \emph{collapsible} pushdown automaton which proceeds by computing the set of traversals of the recursion scheme's computation graph \cite{hmos-lics08}. We conjecture that when the safety constraint is imposed, this encoding can be specialized into a higher-order pushdown automaton (without the collapse operation). Such result would give an alternative proof of Knapik et~al.'s equi-expressivity result \cite{KNU02}.



\subsection*{Algorithmic game semantics}

\emph{Is observational equivalence for \emph{safe} $\ialgol_4$ decidable?} We have seen that up to order $3$, the problem of observational equivalence has the same complexity in the safe finitary fragments as in the unrestricted finitary fragments. At order $4$ the picture remains unclear. Murawski \cite{Murawski2003} showed the undecidability of program equivalence in $\ialgol_i$ for $i\geq4$ by encoding Turing machine computations using finitary $\ialgol_4$ terms.
  Because his encoding relies on unsafe terms, the argument cannot be transposed to
 the safe fragment of \ialgol. The question of whether observational
 equivalence of safe $\ialgol_4$ is decidable thus remains open.

\subsection*{PUR languages}

In this thesis, we have shown that the safety constrained produces
languages whose game semantics enjoy the property that some justification pointers are uniquely recoverable from the underlying sequence of moves. Safe $\ialgol_3$ is an example of language in which \emph{all} pointers are recoverable.
We name this class \emph{PUR} for ``\emph{Pointer Uniquely Recoverable}''. Finitary $\ialgol_2$ (finite base types and no recursion) is the paradigmatic example of a PUR-language (The fact that it is a sublanguage of Safe $\ialgol_3$ is another proof of this fact). But safe fragments are clearly not the only PUR-languages: singleton languages (\ie, containing only one term) are trivial examples of PUR languages. Also the language consisting of all $\ialgol_3$ terms whose beta-reduction is safe is also a PUR language.

A more interesting example is \emph{Serially Re-entrant Idealized  Algol} \cite{abramsky:mchecking_ia}, a version of \ialgol\ where multiple uses of arguments are allowed only if they do not ``overlap in time''.
In the game semantics denotation of a SRIA term
there is at most one pending occurrence of a question at any time.
Each move has therefore a unique justifier and consequently
justification pointers may be ignored. Safe \ialgol\ is not a
sublanguage of SRIA. One reason for this is that none of the two
Kierstead terms $\lambda f . f (\lambda x . f (\lambda y .y ))$ and
$\lambda f . f (\lambda x . f (\lambda y .x ))$ are Serially
Re-entrant whereas the first one is safe. Conversely, SRIA is not a
sublanguage of safe \ialgol\ since the term $\lambda f g. f (\lambda
x . g (\lambda y .x ))$ where $f,g:((o,o),o)$ belongs to SRIA but
not to safe \ialgol.


Another way to generate PUR-languages may consist in constraining types. Joly introduced a notion of ``complexity'' for lambda-terms and proved that there is a constant bounding the complexity of every closed normal lambda-term of a given type $T$
if and only if $T$ can be generated from a finite set of combinators;
Consequently, the only inhabited finitely generated types are the
types of order $\leq 2$ and the types $(A_1, A_2, \ldots, A_n, o)$
such that for all $i = 1..n$: $A_i = o$ , $A_i = o \rightarrow o$ or
$A_i = (o^k \rightarrow o) \rightarrow o$ \cite{DBLP:conf/tlca/Joly01}.
We already know that imposing the first type restriction to Finitary \ialgol\ leads to a
PUR language. Does the second restriction also give a PUR language?

With a view to algorithmic game semantics and its applications, the PUR class is of particular interest.
Indeed, PUR-languages are good candidates of languages with decidable observational equivalence. This is because the simplification of the game semantic model resulting from the nonnecessity of pointers makes the observational equivalence problem more manageable: In \ialgol, for instance one just need to compare the set of complete plays underlying the denotation of a term, forgetting the justification pointers altogether.
For lower-order fragments, a machine characterization of this set is sometimes possible (\eg, finite-state automaton at order-$2$, and deterministic pushdown automata for the order-$3$ fragment with $Y_0$ recursion), subsequently leading to decidability and complexity results for the observational equivalence problem.
