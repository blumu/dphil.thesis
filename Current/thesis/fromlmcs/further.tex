
\section{Further work and open problems}

The safe lambda calculus is still not well understood. Many basic
questions remain. What is a (categorical) model of the safe lambda
calculus? Does the calculus have interesting models?  What kind of
reasoning principles does the safe lambda calculus support, via the
Curry-Howard Isomorphism? Does the safe lambda calculus characterize
a complexity class, in the same way that the simply-typed lambda
calculus characterizes the polytime-computable numeric functions
\cite{DBLP:conf/tlca/LeivantM93}?  Is the addition of unsafe
contexts to safe ones conservative with respect to observational (or
contextual) equivalence?
%Can we obtain a fully abstract model of safe PCF by suitably
%constraining O-moves ({\it i.e.}~``O-incremental justification'')?

With a view to algorithmic game semantics and its applications, it
would be interesting to identify sublanguages of Idealised Algol
whose game semantics enjoy the property that pointers in a play are
uniquely recoverable from the underlying sequence of moves. We name
this class PUR. $\ialgol_2$ is the paradigmatic example of a
PUR-language. Another example is \emph{Serially Re-entrant Idealized
  Algol} \cite{abramsky:mchecking_ia}, a version of \ialgol\ where
multiple uses of arguments are allowed only if they do not ``overlap
in time''.  We believe that a PUR language can be obtained by
imposing the \emph{safety condition} on $\ialgol_3$. Murawski
\cite{Murawski2003} has shown that observational equivalence for
$\ialgol_4$ is undecidable; is observational equivalence for
\emph{safe} $\ialgol_4$ decidable?
